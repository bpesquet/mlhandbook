{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a self-correcting activity generated by [nbgrader](https://nbgrader.readthedocs.io). Fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Run subsequent cells to check your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Cliffworld\n",
    "\n",
    "Adapted from [this blog post](https://davidsanwald.github.io/2016/09/12/RL-tutorial.html).\n",
    "\n",
    "In this activity, you'll train agents to go through a world without falling off cliffs! To do so, you have to complete missing code parts, indicated by `TODOs`.\n",
    "\n",
    "![](images/cliffworld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "assert platform.python_version_tuple() >= (\"3\", \"6\")\n",
    "\n",
    "import random\n",
    "from collections import defaultdict, namedtuple\n",
    "from itertools import product, starmap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, YouTubeVideo\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "sns.set()\n",
    "states_colors = matplotlib.colors.ListedColormap(\n",
    "    ['#9A9A9A', '#D886BA', '#4D314A', '#6E9183'])\n",
    "cmap_default = 'Blues'\n",
    "cpal_default = sns.color_palette((\"Blues_d\"))\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Whole World in 4x12 squares\n",
    "\n",
    "Solving *gridworlds* is a staple of Reinforcement Learning.\n",
    "\n",
    "Here is a plot of our gridworld:\n",
    "\n",
    "![Cliffworld](images/cliff_map.png)\n",
    "\n",
    "The world the agent has to deal with consists of just 4 $\\times$ 12 squares, representing all of its possible **states**. The mechanics of this world are defined by a small number of rules:\n",
    "\n",
    "1. The agent begins each episode on the green square in the upper left corner.\n",
    "2. At each time step the agent can move one step.\n",
    "3. The agent can't leave the board.\n",
    "4. There are two ways an episode can end:\n",
    "    1. The agent reaches the goal state (in black)\n",
    "    2. The agent steps on one of the pink squares. If so, she falls off a cliff.\n",
    "5. Living means suffering. Thus each time step the agent receives a negative reward of **-1**. If the agent falls off the cliff he gets a penalty of **-100**\n",
    "\n",
    "So the basic loop of every agent environment interaction is:\n",
    "\n",
    "```\n",
    "while episode is non terminal: \n",
    "   agent chooses action\n",
    "   environment changes state\n",
    "   agent observes new state and receives reward for new state\n",
    "```\n",
    "\n",
    "The goal of RL is to find the sequence of action which maximizes the expected return over time. In this model world, maximizing the reward (minimizing the negative reward) means finding the shortest path from the start state to the goal in the upper right corner without falling off the cliffs.\n",
    "\n",
    "let's start by defining the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple(\"State\", [\"m\", \"n\"])\n",
    "\n",
    "# World description\n",
    "all_states = [\n",
    "    State(0, 0),\n",
    "    State(0, 1),\n",
    "    State(0, 2),\n",
    "    State(0, 3),\n",
    "    State(0, 4),\n",
    "    State(0, 5),\n",
    "    State(0, 6),\n",
    "    State(0, 7),\n",
    "    State(0, 8),\n",
    "    State(0, 9),\n",
    "    State(0, 10),\n",
    "    State(0, 11),\n",
    "    State(1, 0),\n",
    "    State(1, 1),\n",
    "    State(1, 2),\n",
    "    State(1, 3),\n",
    "    State(1, 4),\n",
    "    State(1, 5),\n",
    "    State(1, 6),\n",
    "    State(1, 7),\n",
    "    State(1, 8),\n",
    "    State(1, 9),\n",
    "    State(1, 10),\n",
    "    State(1, 11),\n",
    "    State(2, 0),\n",
    "    State(2, 1),\n",
    "    State(2, 2),\n",
    "    State(2, 3),\n",
    "    State(2, 4),\n",
    "    State(2, 5),\n",
    "    State(2, 6),\n",
    "    State(2, 7),\n",
    "    State(2, 8),\n",
    "    State(2, 9),\n",
    "    State(2, 10),\n",
    "    State(2, 11),\n",
    "    State(3, 0),\n",
    "    State(3, 1),\n",
    "    State(3, 2),\n",
    "    State(3, 3),\n",
    "    State(3, 4),\n",
    "    State(3, 5),\n",
    "    State(3, 6),\n",
    "    State(3, 7),\n",
    "    State(3, 8),\n",
    "    State(3, 9),\n",
    "    State(3, 10),\n",
    "    State(3, 11),\n",
    "]\n",
    "\n",
    "cliff_states = all_states[1:11]  # pink squares (to be avoided)\n",
    "goal_state = State(m=0, n=11)  # black square\n",
    "start_state = State(m=0, n=0)  # white square\n",
    "\n",
    "terminal = cliff_states + [goal_state]  # terminal states\n",
    "dflt_reward = -1\n",
    "cliff_reward = -100\n",
    "\n",
    "# Possible actions\n",
    "moves = {\">\": State(0, 1), \"v\": State(1, 0), \"<\": State(0, -1), \"^\": State(-1, 0)}\n",
    "\n",
    "parameters = {\n",
    "    \"all_states\": all_states,\n",
    "    \"cliff_states\": cliff_states,\n",
    "    \"goal_state\": goal_state,\n",
    "    \"start_state\": start_state,\n",
    "    \"terminal\": terminal,\n",
    "    \"dflt_reward\": dflt_reward,\n",
    "    \"cliff_reward\": cliff_reward,\n",
    "    \"moves\": moves,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's possible to actually plot the map of the cliffworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.full((4,12), -1, dtype='int64')\n",
    "color = np.zeros((4,12))\n",
    "for x in all_states:\n",
    "    if x == start_state:\n",
    "        rewards[x] = -1\n",
    "        color[x] = 4\n",
    "    if x == goal_state:\n",
    "        rewards[x] = -1\n",
    "        color[x] = 2\n",
    "    if x in cliff_states:\n",
    "        rewards[x] = -100\n",
    "        color[x] = 1\n",
    "\n",
    "ax = sns.heatmap(color, cmap=states_colors, annot=rewards, cbar=False, square=True, linewidths=1, fmt='' )\n",
    "ax.set_title('Cliffworld States\\n')\n",
    "ax.set(ylabel='m', xlabel='n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeddf7228f3c578411d288b6a3eb8546",
     "grade": true,
     "grade_id": "cell-e3d56705e9492a65",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CliffWorld:\n",
    "    \"\"\"Cliffworld domain for RL.\n",
    "\n",
    "    A simple domain with 4 x 12 = 48 possible discrete states.\n",
    "    Originally from Sutton and Barto.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Specified in the dictionary above because this class is not\n",
    "    intended for use outside this notebook.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *initial_data, **kwargs):\n",
    "        for dictionary in initial_data:\n",
    "            for key in dictionary:\n",
    "                setattr(self, key, dictionary[key])\n",
    "        for key in kwargs:\n",
    "            setattr(self, key, kwargs[key])\n",
    "        self.record_list = []\n",
    "        self.position = [self.start_state]  # List of agent positions\n",
    "        self.log_dict = {}\n",
    "        self.reward_sum = 0\n",
    "\n",
    "    def newstate(self, state, action):\n",
    "        \"\"\"Computes the newstate.\n",
    "\n",
    "        Takes a state and an action from the agent and computes its next position.\n",
    "\n",
    "        Args:\n",
    "            state: a tuple (m, n)  representing the coordinates of the current state\n",
    "            action: index of an action\n",
    "\n",
    "        Returns:\n",
    "            newstate: a tuple (m, n) representing the coordinates of the new position\n",
    "\n",
    "        \"\"\"\n",
    "        move = moves[action]\n",
    "        newstate = State(state.m + move.m, state.n + move.n)\n",
    "        self.position.append(newstate)\n",
    "        return newstate\n",
    "\n",
    "    def reward(self, state):\n",
    "        \"\"\"Computes the reward signal for a given state.\n",
    "\n",
    "        Takes a state and checks if it's a cliff or just a normal state.\n",
    "\n",
    "        Args:\n",
    "            state: a named tuple (m, n) \n",
    "                representing the coordinates of the current state.\n",
    "\n",
    "        Returns:\n",
    "            reward: a scalar value. -100 for a cliff state, -1 otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "        if state in self.cliff_states:\n",
    "            reward = self.cliff_reward\n",
    "        else:\n",
    "            reward = self.dflt_reward\n",
    "        self.reward_sum += reward\n",
    "        return reward\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"Checks if state is terminal.\n",
    "\n",
    "        If the agent reached its goal or fell off a cliff the episode ends.\n",
    "        Otherwise it will continue.\n",
    "\n",
    "        Args:\n",
    "            state: namedtuple, State(m, n), representing position.\n",
    "\n",
    "        Returns:\n",
    "            True if state is terminal, False otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "        if state in self.terminal:\n",
    "            self.log_stuff(state)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def log_stuff(self, state):\n",
    "        \"\"\"Log things for analysis.\"\"\"\n",
    "        self.position.append(state)\n",
    "        self.log_dict[\"visited_states\"] = self.position[:]\n",
    "        self.log_dict[\"reward\"] = self.reward_sum\n",
    "        self.record_list.append(self.log_dict)\n",
    "        self.log_dict = {}\n",
    "        self.position = [self.start_state]\n",
    "        self.log_dict = {}\n",
    "        self.reward_sum = 0\n",
    "\n",
    "    def valid_actions(self, state):\n",
    "        \"\"\"Compute valid actions for given state.\"\"\"\n",
    "\n",
    "        valid_actions = []\n",
    "        if (state.m + moves[\">\"].m, state.n + moves[\">\"].n) in self.all_states:\n",
    "            valid_actions.append(\">\")\n",
    "        # TODO: add other valid actions for the state\n",
    "        # YOUR CODE HERE\n",
    "        return valid_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent\n",
    "\n",
    "A good way to understand how environment and agent interact is to write a simple agent which just chooses actions at random. Even though the agent does not learn anything at all, the interface to the environment will be the same for the Q-Learning agent following next.\n",
    "\n",
    "The agent needs only 2 methods. One chooses and executes an action in the environment, the other one learns from the reward following this action and the resulting new state. Of course the learning method of the random agent doesn't do anything at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fe151893952972813e53c4424eaca67",
     "grade": true,
     "grade_id": "cell-754aaea3a8a1af2b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"Just choosing actions at random. Learns nothing.\n",
    "\n",
    "    We write this for the purpose of understanding the interface between\n",
    "    agent and domain we later use for the real RL-agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reward_sum = 0\n",
    "\n",
    "    def act(self, state, valid_actions):\n",
    "        \"\"\"Take a random action among valid ones. Returns action.\"\"\"\n",
    "        # TODO\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def learn(self, state, action, newstate, reward):\n",
    "        \"\"\"Never learns anything\"\"\"\n",
    "        self.reward_sum += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this random agent and the Cliffworld domain class are instantiated. Their loop of interaction is exactly the same as the loop described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some agent randomly roaming through a\n",
    "# gridlike domain with 4x12 discrete states.\n",
    "# Occasionally jumping off cliffs.\n",
    "\n",
    "random_agent = RandomAgent()  #: Instantiate an agent object\n",
    "domain = CliffWorld(parameters)  #: Create our small world\n",
    "\n",
    "\n",
    "def run_episode(domain, agent):\n",
    "    state = domain.start_state\n",
    "    while not domain.is_terminal(state):\n",
    "        # Get a list of allowed moves for the current state\n",
    "        valid_actions = domain.valid_actions(state)\n",
    "        # Take the current state as input and compute an action\n",
    "        action = agent.act(state, valid_actions)\n",
    "        # Take the action and compute the changed state\n",
    "        newstate = domain.newstate(state, action)\n",
    "        # Compute reward for new state\n",
    "        reward = domain.reward(newstate)\n",
    "        # Learn\n",
    "        agent.learn(state, action, newstate, reward)\n",
    "        # Newstate becomes the current state for next iteration\n",
    "        state = newstate\n",
    "\n",
    "\n",
    "run_episode(domain, random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve visited states\n",
    "agent_position_log = domain.record_list[-1][\"visited_states\"]\n",
    "\n",
    "agent_path = np.empty((4, 12), dtype=str)\n",
    "for state_visited in agent_position_log:\n",
    "    if state_visited in terminal:\n",
    "        agent_path[state_visited] = \"X\"\n",
    "    else:\n",
    "        agent_path[state_visited] = \"O\"\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_title(\"Path of random agent\\n\")\n",
    "figure = sns.heatmap(\n",
    "    color,\n",
    "    annot=agent_path,\n",
    "    fmt=\"\",\n",
    "    ax=ax,\n",
    "    cbar=False,\n",
    "    linewidth=1,\n",
    "    cmap=states_colors,\n",
    "    square=True,\n",
    ")\n",
    "figure.set(xlabel=\"n\", ylabel=\"m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly just acting randomly doesn't get the agent very far or even worse, sooner or later he dies by falling off a cliff when he steps on one of the pink tiles representing terminal states.\n",
    "\n",
    "Let's try something else.\n",
    "\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "The goal is to find a sequence of actions which maximizes the expected return of rewards in the gridworld. \n",
    "So maybe it is enough to learn from experience how \"good\" it is to take some action in a particular state.\n",
    "If it's possible to learn a function which expresses this by assigning a value to every state action pair it's possible to just take the highest valued action in every state and finally find the goal.\n",
    "\n",
    "But how?\n",
    "\n",
    "For every action the agent receives a reward. Sometimes there's a discount on this reward, so that that reward on the next step is favorable to a reward in the distant future.\n",
    "At time step $t$ the return $G_t$ is the sum of all future rewards from that time step on\n",
    "\n",
    "$$G_{t} = r_{t+1}+r_{t+2}+r_{t+3}+\\dots +r_{n}$$\n",
    "\n",
    "In the discounted case, rewards get multiplied with some $\\gamma<0$ raised to the power of the number of time steps this reward is away from the next reward. So if a reward is $k$ time steps aways it's discounted by $\\gamma^{k-1}$:\n",
    "\n",
    "$$ G_{t} = r_{t+1}+ \\gamma r_{t+2}+ \\gamma^2 r_{t+3}+ \\dots  = \\sum_{k=0}^{\\infty}\\gamma^k r_{t+k+1} $$\n",
    "\n",
    "Because the future is unknown, the return has to be expressed in a different way. It is possible to understand the return as the sum of just two parts:\n",
    "\n",
    "$$G_{t}= r_{t+1} + \\gamma G_{t+1}$$\n",
    "\n",
    "The return at time stept $t$ is the immediate reward on the next time step $r_{t+1}$ plus the discounted return from this time step on.\n",
    "\n",
    "This can be used to introduce a function, $Q(s,a)$ , which assigns some value to every possible action in each state which tells the agent how good this action is in terms of maximizing the return $G_{t}$.\n",
    "\n",
    "The Q function can be split in the same manner as the return $G_{t}$. The maximum value for taking some action $a$ in state $t$ is the sum of the reward for taking that action and the maximum (discounted) value for taking the optimal action on the next time step. \n",
    "\n",
    "$$Q(s_t,a_t)= r_{t+1} + \\gamma max_{a_{t+1}} Q(s_{t+1}, a_{t+1})$$\n",
    "\n",
    "The agent has to learn $Q(s, a)$ which maps all state action pairs to a value which has to be as close as possible to the *true* value of a state action pair. If a good estimate of the true Q-values is known, the agent just has to take the highest valued action in every state.\n",
    "This leads to the learning rule of the Q-learning algorithm (the use of capital letters indicates actual tabular values):\n",
    "\n",
    "$$Q(S_{t}A_{t})\\gets Q(S_{t}A_{t})+\\alpha[R_{t+1}+\\gamma \\max_{A}Q(S_{t+1},A)-Q(S_{t},A_{t})]$$\n",
    "\n",
    "The agent has some estimate of the value of taking action $A_t$ in state $S_t$. Now he executes this action and receives the reward $R_{t+1}$ in return. And because of the definition above  $Q(s_t,a_t)$ can be updated with the new information. The direction of the update is determined by the immediate reward the agent receives plus the (discounted) difference between the maximum of the estimate $Q(S_{t+1})$ of the state we now reached times some small step size parameter $\\alpha$.\n",
    "\n",
    "A simple intuition for this:\n",
    "\n",
    "Because the value of a state action pair $Q(S_t,A_t)$ can be estimated as the reward $R_{t+1}$ for taking that action plus the estimated value $Q(S_{t+1},A_{t+1})$ for taking the best action on the next time step, we can update our estimate of $Q(S_t,A_t)$ when we receive the actual reward $R_{t+1}$ on the next time step.\n",
    "\n",
    "If you think you'll need 60 minutes for the way from your home to your workplace but you get in a traffic jam after 10 minutes you already know you'll be late and can call your boss before actually arriving. \n",
    "The closer you get to your workplace, the smaller your updates to your estimated arrival will get.\n",
    "If you're at the first floor but notice the elevator isn't working that day, having to take the stairs does not change the time you estimated for the way from home to work anymore.\n",
    "\n",
    "This part is also called $[R_{t+1}+\\gamma \\max_{a}Q(S_{t+1},a)-Q(S_{t},A_{t})]$ or the Temporal Difference error.\n",
    "Actually this TD-error could also be called *surprise*. It's an expressions of the difference between an actual experience and the expectation preceding this experience. The intuition that the amount of surprise and learning are closely related is indeed congruent to some results of neuroscientific [research](http://link.springer.com/article/10.3758/BF03196058).\n",
    "\n",
    "Updating expectations by the difference to actual experience results in the following algorithm:\n",
    "\n",
    "```\n",
    "initialize values Q[states, actions] arbitrarily (as zero)\n",
    "begin at start state S\n",
    "    while episode is non terminal:\n",
    "        execute action A with highest Q-Value\n",
    "        observe reward R' and newstate S'\n",
    "        Q[S, A] = Q[S', A']+ alpha * [R'+ gamma.max_a[Q[S', A'] - Q[S, A]]\n",
    "        S = S'\n",
    "        \n",
    "```\n",
    "\n",
    "Because the agent doesn't know anything yet, all Q-values are initialized as zero. This could be an array with the size $possible\\, states \\times actions$. For the cliffworld, the array would consist of $(4 \\times 12) \\times 4$ tiles because the agent can go *left*, *right*, *up* or *down*.\n",
    "Often the number of possible states isn't known beforehand, so Python's *defaultdict* keyed by a nested tuple in the form (action (row-index, colum-index)) could be used instead of an array. Whenever there's no existing value for a state/action-key the dictionary returns 0.\n",
    "\n",
    "There's one more thing to know and that's $\\epsilon-greedy$ action selection.\n",
    "If someone exploits just the knowledge he already possesses and never tries anything new, he also won't be able to discover or *learn* anything new.\n",
    "So the agent selects her actions $\\epsilon-greedy$: In a proportion of $1-\\epsilon$ times he doesn't select the action with the highest value and just chooses one at random instead. The value of $\\epsilon$  is decreased over time as the experience grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60580a614515ec098729e3176300c18a",
     "grade": true,
     "grade_id": "cell-44310477859d4bee",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    \"\"\"Q-Learning agent.\n",
    "\n",
    "    A value based, off-policy RL-agent.\n",
    "    Uses defaultdict to return 0 for unknown state/action pairs.\n",
    "    This has the same purpose as initializing all unknown states to zero.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        epsilon: Parameter for epsilon-greedy action selection.\n",
    "        epsilon_modifier: scalar value between 0 and 1,\n",
    "        decay factor for epsilon\n",
    "        alpha: Learning rate or stepsize. Usually <<1\n",
    "        gamma: discount of future rewards <=1\n",
    "\n",
    "    Attrs:\n",
    "        Q: Python defaultdict with defaultvalue 0 containing\n",
    "        the Q-values for state action pairs.\n",
    "        Keyed by a nested state action tuple.\n",
    "        Example:\n",
    "        {\n",
    "            (State(m=0, n=0), '>'): -99.94360791266038,\n",
    "            (State(m=0, n=0), 'v'): -1.1111111111107184,\n",
    "            (State(m=1, n=0), '>'): -1.111111111107987,\n",
    "            (State(m=1, n=0), 'v'): -1.1111111111079839,\n",
    "            (State(m=1, n=0), '^'): -1.111111111108079\n",
    "        }\n",
    "        A: Defaultdict keyed by state tuple, values\n",
    "        are keys of executed actions. Is used to determine\n",
    "        whether an action in a state took place in the past.\n",
    "        If there's no memory len(A[state]) == 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha, epsilon, gamma):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(int)\n",
    "        self.A = defaultdict(set)\n",
    "        self.td_list = []  # List of TD errors\n",
    "\n",
    "    def act(self, state, valid_actions):\n",
    "        \"\"\"Choose action.\n",
    "\n",
    "        Take state tuple and valid actions and choose action.\n",
    "        Action can either be random or greedy to ensure exploration.\n",
    "        Probability of selecting random actions depends on epsilon.\n",
    "        Smaller epsilon means less randomness.\n",
    "\n",
    "        Args:\n",
    "            state: state tuple (n,m) describes current position.\n",
    "            valid_actions: list of indices of valid actions for a state.\n",
    "\n",
    "        Returns:\n",
    "            action: Index of selected action.\n",
    "            Can either be chosen at random or greedily.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: act greedy if a random number in [0,1] is > epsilon, randomly otherwise\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def learn(self, state, action, newstate, reward):\n",
    "        \"\"\"Compute update of Q-function.\n",
    "\n",
    "        Update Q-values when reaching new state and receiving reward.\n",
    "        New value equals to the old value + the computed td-error times\n",
    "        the learning rate alpha.\n",
    "        Also adds the executed action to A, to keep track of all state\n",
    "        action pairs.\n",
    "\n",
    "        Args:\n",
    "            state: namedtuple (m, n) the state of the last timestep.\n",
    "            action: index of executed action at the last timestep.\n",
    "            newstate: current state reached after executing action.\n",
    "            reward: scalar value received for reaching newstate.\n",
    "        \"\"\"\n",
    "        # TODO: update Q-Value for state and action\n",
    "        # Syntax: self.Q[state, action] = ...\n",
    "        # YOUR CODE HERE\n",
    "        self.A[state].add(action)\n",
    "\n",
    "    def act_random(self, valid_actions):\n",
    "        \"\"\"Choose index of action from valid actions at random.\n",
    "\n",
    "        Called either if epsilon greedy policy returns random\n",
    "        or if there's no previous knowledge of action values for\n",
    "        a state.\n",
    "\n",
    "        Args:\n",
    "            valid: List of indices of valid actions for a state.\n",
    "\n",
    "        Returns:\n",
    "            action: Index of selected action.\n",
    "                Can either be chosen at random or greedy.\n",
    "\n",
    "        \"\"\"\n",
    "        return random.choice(valid_actions)\n",
    "\n",
    "    def act_greedy(self, state, valid_actions):\n",
    "        \"\"\"Choose action with the highest Q-value.\n",
    "\n",
    "        First checks whether the agent previously has executed any actions at all\n",
    "        in the current state. If not it calls the random act_random method.\n",
    "\n",
    "        Args:\n",
    "            valid_actions: List of indices of valid actions for a state.\n",
    "            state: namedtuple, State(n, m),\n",
    "                representing coordinates of the current state.\n",
    "\n",
    "        Returns:\n",
    "            chosen_action: Index of selected action.\n",
    "                Can either be chosen at random (for initial state) or greedy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.A[state]) == 0:\n",
    "            chosen_action = self.act_random(valid_actions)\n",
    "        else:\n",
    "            q_s = {actions: self.Q[state, actions] for actions in self.A[state]}\n",
    "            chosen_action = max(q_s, key=q_s.get)\n",
    "        return chosen_action\n",
    "\n",
    "    def td_error(self, state, action, newstate, reward):\n",
    "        \"\"\"Compute td error to update value dict\n",
    "\n",
    "        First checks wether the agent previously as executed any actions at all in\n",
    "        the current state. If not the maximum Q-value for that state defaults to 0.\n",
    "        It fetches Q-values for all previously exectued actions in newstate tracked in A.\n",
    "        Next computes the key of the largest Q-value and queris the fetches Q-values.\n",
    "        Finally computes the td error for the learning update.\n",
    "\n",
    "        Args:\n",
    "            state: state of the last timestep.\n",
    "            action: index of selected action in that state\n",
    "            newstate: state the agent just reached.\n",
    "            reward: scalar value\n",
    "\n",
    "        Returns:\n",
    "            td error.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.A[newstate]) == 0:\n",
    "            max_qval = 0\n",
    "        else:\n",
    "            q_vals = {\n",
    "                actions: self.Q[newstate, actions] for actions in self.A[newstate]\n",
    "            }\n",
    "            max_qval_key = max(q_vals, key=q_vals.get)\n",
    "            max_qval = q_vals[max_qval_key]\n",
    "        # TODO: compute TD error, add it to TD error list attribute and return it\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The only thing missing to finally test our Q-Agent is a function to conduct multiple episodes.\n",
    "As the agent progresses, the amount of exploration by acting randomly is gradually reduced on each episode by modifying epsilon with a decay parameter. For evaluation purposes, in the last episode the agent exploits the learned value function without any exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(domain, agent, epsilon_decay, n_episodes):\n",
    "    for i in range(n_episodes):\n",
    "        agent.epsilon *= epsilon_decay\n",
    "        run_episode(domain, agent)\n",
    "    print(\n",
    "        \"Setting epsilon parameter to zero\",\n",
    "        \"to prevent random actions and evaluate learned policy.\\n\",\n",
    "    )\n",
    "    agent.epsilon = 0\n",
    "    run_episode(domain, agent)\n",
    "    last_reward = domain.record_list[-1][\"reward\"]\n",
    "    print(\n",
    "        \"Trained for {0} episodes.\\n\"\n",
    "        \"\\nGained reward of {1} points in the last episode.\".format(\n",
    "            n_episodes, last_reward\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c6738b620fc1a80020d9cef338aa7d6",
     "grade": true,
     "grade_id": "cell-286e16c79b27877f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epsilon = 0.9\n",
    "epsilon_decay = 0.99\n",
    "gamma = 0.9\n",
    "alpha = 0.25\n",
    "n_episodes = 500\n",
    "\n",
    "# TODO: using hyperparameters, implement QAgent and Cliffworld, then run experiment\n",
    "# YOUR CODE HERE\n",
    "\n",
    "logged_data = domain.record_list\n",
    "Q_table = q_agent.Q\n",
    "A_table = q_agent.A\n",
    "td = q_agent.td_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code visualizes what the agent has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_array = np.empty((4, 12), dtype=str)\n",
    "value_array = np.empty((4, 12), dtype=float)\n",
    "for state in domain.all_states:\n",
    "    if len(A_table[state]) == 0:\n",
    "        chosen_action = \"c\"\n",
    "    else:\n",
    "        q_s = {actions: Q_table[state, actions] for actions in A_table[state]}\n",
    "        chosen_action = max(q_s, key=q_s.get)\n",
    "        max_qval = q_s[chosen_action]\n",
    "    action_array[state] = chosen_action\n",
    "    value_array[state] = max_qval\n",
    "    action_array[(0, 11)] = \"g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = sns.heatmap(value_array, annot = action_array,  fmt= '', square=True, cbar=False, cmap= cmap_default)\n",
    "ax = plt.axes()\n",
    "ax.set_title('Final Q-function and resulting policy\\n')\n",
    "figure.set(xlabel='n', ylabel='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final path of the agent. By tweaking some of the parameters it's easy to find a combination which converges to the shortest path (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_position_log = domain.record_list[-1]['visited_states']\n",
    "\n",
    "agent_path = np.empty((4, 12), dtype=str)\n",
    "for state_visited in agent_position_log:\n",
    "    if state_visited in terminal:\n",
    "        agent_path[state_visited] = 'X'\n",
    "    else:\n",
    "        agent_path[state_visited] = 'O'\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_title('Path Q-agent final episode \\n')\n",
    "figure = sns.heatmap(color,\n",
    "                     annot=agent_path,\n",
    "                     fmt='',\n",
    "                     ax=ax,\n",
    "                     cbar=False,\n",
    "                     linewidth=1,\n",
    "                     cmap=states_colors,\n",
    "                     square=True)\n",
    "figure.set(xlabel='n', ylabel='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Update the cliffworld rewards and retrain your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
